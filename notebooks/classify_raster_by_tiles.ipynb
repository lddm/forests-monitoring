{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AENXq97iaCjC"
   },
   "source": [
    "## Introduction\n",
    "This notebook implements a solution for the Kaggle Planet: [Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space) challenge by finetunning a ResNet50 previously trained in ImageNet. \n",
    "\n",
    "Sections and code are ordered in a way that should be easy to follow, if you have doubts or comments please do not hesitate in reaching me by opening an issue in my ml-journey repository: https://github.com/lddm/ml-journey  \n",
    "\n",
    "I use this notebook for both training and applying the classifier for the challenge. The latest state of the notebook in github may not have all cells' outputs. If you have doubts about some output you can always try running the notebook end-to-end on your own or ask me, I would be happy to help.\n",
    "\n",
    "With the latest classifier trained in this notebook a score of `0.89188` was obtained, the top score in the challenge leaderboard is `0.93317`. Although the score is not yet at the same level as the top scores, I believe the obtained classifier is sufficiently good for the application of forest monitoring I plan to work on next. I may get back to work on improving this classifier in the future, in that case, I believe the following points are worth reviewing:\n",
    "* The neural network may be overfitting the training data and may benefit from including a weight-decay in the loss while training.\n",
    "* I'm using Colab free infrastructure for training and classification. Currently training is somewhat limited by the interaction of GPU speed and memory and the timeout Google set when using the Colab free environment. There's some potential benefit of using TPUs instead. To the best of my knowledge, the easiest way to make the switch would be to move from using PyTorch to Pytorch-Lighting. PyTorch-Lighting provides a lot of extra benefits and was recommended by several colleagues and friends, I definitely want to try it out in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PuzQV0zurbW"
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# To change Jupyter notebook to a dark theme the following command can be issued in a terminal before launching\n",
    "# jupyter notebook server\n",
    "! jt -t onedork -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Oohq-EzU3X_6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import telluric as tl\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import preprocess_raster_image\n",
    "\n",
    "# Constants\n",
    "TRAIN_DIR = 'train/train-jpg'\n",
    "TEST_DIR = 'test/test-jpg'\n",
    "# ROOT_PATH = '/content/gdrive/MyDrive/planet_amazon'\n",
    "ROOT_PATH = '.'\n",
    "MODEL_PATH = os.path.join(ROOT_PATH, 'planet_challenge_model_one_cycle_lr_v2.tar')\n",
    "TRAINING_BATCH_SIZE = 64\n",
    "VALIDATION_BATCH_SIZE = 64\n",
    "TRAINING_EPOCHS = 5\n",
    "SUBMISSION_FILENAME = 'submission_one_cycle_lr_data_augmentation.csv'\n",
    "CLR_PICKLE_PATH = 'clr_test.bin'\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autVzv_Il-E9"
   },
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-vPUakAO9iDc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_items(df):\n",
    "    return df\n",
    "    # Grab subset of items\n",
    "    # return df.sample(n=64*30, random_state=891237)\n",
    "\n",
    "def get_x(df_row):\n",
    "    img_name = df_row['image_name']\n",
    "    img_path = os.path.join(TRAIN_DIR, f'{img_name}.jpg')\n",
    "    return img_path\n",
    "\n",
    "def get_y(df_row):\n",
    "    return df_row['tags'].split(' ')\n",
    "\n",
    "def imshow(inp, fig_size=4, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "    ax.imshow(inp)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "def initialize_resnet(num_classes, use_pretrained=True):\n",
    "    model = models.resnet50(pretrained=use_pretrained)\n",
    "    # Adjust last fully connected layer to number of classes in the PlanetAmazonChallenge\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# Based on https://www.kaggle.com/igormq/f-beta-score-for-pytorch\n",
    "def f2_score(y_true, y_pred, threshold=0.5):\n",
    "    return fbeta_score(y_true, y_pred, 2, threshold)\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=1)\n",
    "    precision = true_positive.div(y_pred.sum(dim=1).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=1).add(eps))\n",
    "\n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2))\n",
    "\n",
    "# Based on https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "# Allows obtaining image paths when iterating by batches using the DataLoader\n",
    "# Code is modified to only return the file name, without path and without \n",
    "# extension, thus generating the output format expected by the Kaggle \n",
    "# competition\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        img_path = self.imgs[index][0]\n",
    "        img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (img_name,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hWGgYVU9uoF"
   },
   "source": [
    "The [torchvision.datasets](https://pytorch.org/vision/0.8/datasets.html#torchvision-datasets) class expects a specific structure for the data and their corresponding labels, e.g: ![Screenshot from 2021-02-27 16-32-20.png](data:image/png;base64,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)\n",
    "\n",
    "This is not very convenient for handling how data is provided in the Kaggle _Understanding the Amazon from Space_ challenge. This is easily fixable by implementing a custom sub-class of the torch Dataset class. Code below is based on https://www.kaggle.com/mratsim/starting-kit-for-pytorch-deep-learning#Loading-the-data---first-part---DataSet  \n",
    "\n",
    "The implemented `KaggleAmazonDataset` makes use of [sklearn.preprocessing.MultiLabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn-preprocessing-multilabelbinarizer) to encode the categorical labels in a convenient numerical format for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7b61FspM-_Ov",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class KaggleAmazonDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        Transform (optional) object containing transformations to apply on imagery.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None, number_samples=None, check_corrupt_files=True):\n",
    "    \n",
    "        self.tmp_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "        \n",
    "        if check_corrupt_files:\n",
    "            self._check_corrupt_files()\n",
    "\n",
    "        image_names = self.tmp_df['image_name']\n",
    "        tags = self.tmp_df['tags']\n",
    "        if number_samples:\n",
    "          image_names = image_names[:number_samples]\n",
    "          tags = tags[:number_samples]\n",
    "          self.dataset_size = number_samples\n",
    "        else:\n",
    "          self.dataset_size = len(image_names)\n",
    "\n",
    "        self.X_train = image_names\n",
    "        # self.y_train is a sparse-matrix of size num_samples x num_classes where an element [i,j] equals 1 \n",
    "        # iff the sample with index 'i' correspond to class 'j'\n",
    "        self.y_train = self.mlb.fit_transform(tags.str.split()).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_path, self.X_train[index] + \n",
    "                                      self.img_ext))\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "\n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "    \n",
    "    def _check_corrupt_files(self):\n",
    "        # check that all images listed in the train.csv are available on the training folder\n",
    "        assert self.tmp_df['image_name'].apply(lambda x: os.path.isfile(os.path.join(\n",
    "                self.img_path, x + self.img_ext))).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        # some files available in the folder are corrupted causing an PIL.UnidentifiedImageError\n",
    "        for image_name in self.tmp_df['image_name']:\n",
    "          file_size = os.stat(os.path.join(\n",
    "              self.img_path, image_name + self.img_ext)).st_size\n",
    "          if file_size == 0:\n",
    "            raise(OSError('File {} is corrupt'.format(image_name)))\n",
    "        \n",
    "    \n",
    "    def decode_binary_label(self, array):\n",
    "      return self.mlb.inverse_transform(array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD-e-X03yNPN"
   },
   "source": [
    "## Load dataset and split it into training and validation sub-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_wAZz8rDg4rW",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# the parameters used here are based on https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "data_transforms = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      # Normalization mean and std. dev. \n",
    "                                      # factors are taken from ImageNet\n",
    "                                      transforms.Normalize(\n",
    "                                          [0.485, 0.456, 0.406],\n",
    "                                          [0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = KaggleAmazonDataset('train_v2.csv', TRAIN_DIR, '.jpg', \n",
    "                                    data_transforms, check_corrupt_files=False)\n",
    "\n",
    "dataset_validation_ratio = 0.2\n",
    "dataset_validation_size = int(dataset_validation_ratio * len(dataset))\n",
    "dataset_train_size = len(dataset) - dataset_validation_size\n",
    "dataset_train, dataset_valid = random_split(dataset, (dataset_train_size, dataset_validation_size))\n",
    "\n",
    "train_loader = DataLoader(dataset_train,\n",
    "                          batch_size=TRAINING_BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1, # 1 for CUDA\n",
    "                          pin_memory=True # CUDA only\n",
    "                         )\n",
    "\n",
    "val_loader = DataLoader(dataset_valid,\n",
    "                        batch_size=VALIDATION_BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=1, # 1 for CUDA\n",
    "                        pin_memory=True # CUDA only\n",
    "                        )\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "\n",
    "dataset_sizes = {'train': len(dataset_train),\n",
    "                 'val': len(dataset_valid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bna5u9L91I-L",
    "outputId": "31330c9d-9f50-45da-eb2f-5756d16cbe63",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': 32384, 'val': 8095}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAhTMDdg4rW"
   },
   "source": [
    "## Test train_loader and classes decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D_iTuaHHJKpN",
    "outputId": "86b56e90-b061-4e51-8e15-0cc73901e1f7",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7594ca1d8f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Make a grid from batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 330, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-4-64beaf433caa>\", line 39, in __getitem__\n    self.img_ext))\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/PIL/Image.py\", line 2912, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: 'train/train-jpg/train_34012.jpg'\n"
     ],
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 330, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-4-64beaf433caa>\", line 39, in __getitem__\n    self.img_ext))\n  File \"/home/lddm/Development/forests-monitoring/.venv_37_classification/lib/python3.7/site-packages/PIL/Image.py\", line 2912, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: 'train/train-jpg/train_34012.jpg'\n",
     "output_type": "error"
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs, nrow=4)\n",
    "\n",
    "imshow(out, fig_size=20)\n",
    "\n",
    "train_dataset.decode_binary_label(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FETH6Ok56ad-"
   },
   "source": [
    "## Finetunning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq_MUszb6NHB",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, one_cycle=None, \n",
    "                last_epoch=0, num_epochs=25, save_model=False, threshold=0, \n",
    "                epoch_loss_evolution={}, epoch_acc_evolution=[]):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    if epoch_acc_evolution:\n",
    "      best_acc = max(epoch_acc_evolution)\n",
    "    else:\n",
    "      best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(last_epoch, num_epochs):\n",
    "        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        # print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            with tqdm(dataloaders[phase], unit=\"batch\", \n",
    "                      desc='Epoch {}/{}'.format(epoch, num_epochs - 1)) as training_batch:\n",
    "              for inputs, labels in training_batch:\n",
    "                  inputs = inputs.to(device)\n",
    "                  labels = labels.to(device)\n",
    "\n",
    "                  if phase == 'train' and one_cycle:\n",
    "                    lr, mom = one_cycle.calc()\n",
    "                    update_lr(optimizer, lr)\n",
    "                    update_mom(optimizer, mom)\n",
    "\n",
    "                  # zero the parameter gradients\n",
    "                  optimizer.zero_grad()\n",
    "\n",
    "                  # forward\n",
    "                  # track history if only in train\n",
    "                  with torch.set_grad_enabled(phase == 'train'):\n",
    "                      outputs = model(inputs)\n",
    "                      preds = outputs > threshold\n",
    "                      loss = criterion(outputs, labels)\n",
    "\n",
    "                      # backward + optimize only if in training phase\n",
    "                      if phase == 'train':\n",
    "                          loss.backward()\n",
    "                          optimizer.step()\n",
    "\n",
    "                  # statistics\n",
    "                  running_loss += loss.item() * inputs.size(0)\n",
    "                  running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # print('f2beta score: ', fbeta_score(labels.data, preds, 2, 0.2))\n",
    "            if phase == 'train' and one_cycle is None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss_evolution[phase].append(running_loss \n",
    "                                               / dataset_sizes[phase])\n",
    "            \n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_acc_evolution.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss_evolution[phase][-1], epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if save_model:\n",
    "                  torch.save({\n",
    "                      'epoch': epoch,\n",
    "                      'model_state_dict': model.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      'loss_evolution': epoch_loss_evolution,\n",
    "                      'accuracy_evolution': epoch_acc_evolution\n",
    "                      }, MODEL_PATH)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best train Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_QAzlx56NHF",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = initialize_resnet(len(dataset.mlb.classes_), use_pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# Loss selected for multi-label classification problem based on https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/45\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-7)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 8 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "# Initialize OneCycle object\n",
    "number_iterations = TRAINING_EPOCHS * TRAINING_BATCH_SIZE\n",
    "one_cycle = OneCycle(number_iterations, max_lr=4e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ao9TtWOl2LfR",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "  checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  last_epoch = checkpoint['epoch']\n",
    "  epoch_loss_evolution = checkpoint['loss_evolution']\n",
    "  epoch_acc_evolution = checkpoint['accuracy_evolution']\n",
    "else:\n",
    "  epoch_loss_evolution = {\n",
    "      'train': [],\n",
    "      'val': []\n",
    "  }\n",
    "  epoch_acc_evolution = []\n",
    "  last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1SZzsA27a7k",
    "outputId": "6bfe76e1-ed4b-4a94-8cc4-827692cd90f1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, one_cycle,\n",
    "                    last_epoch=last_epoch, num_epochs=TRAINING_EPOCHS, save_model=True, \n",
    "                    threshold=0.2, epoch_loss_evolution=epoch_loss_evolution, \n",
    "                    epoch_acc_evolution = epoch_acc_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "9QgvuSpY-R9C",
    "outputId": "63aafeeb-6bc6-4b0b-c961-812a2707f09a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_evolution['train'], label='Training loss')\n",
    "plt.plot(epoch_loss_evolution['val'], label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C4jN8P0AKhu"
   },
   "source": [
    "# Classify test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA1qikcakkjH"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "83d2a0b388984de4b394dbc2dc59e5af",
      "1c8424e4aae5485199b53d7a2443034e",
      "605fef7441184dedb805816f48039fa8",
      "58a5037696124d84844fc92cf8552503",
      "aa023ba766824a54823e71f6eb42636f",
      "83b141877a0c4de98d007704fed029f6",
      "5f98920151ee46e59a405fd41038af3e",
      "566eb001b14c43678ebe8c281a8662c4"
     ]
    },
    "id": "WpkYvtRbFXp4",
    "outputId": "5dafe53a-f0b5-493f-e8ce-5bfab10892ef",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = initialize_resnet(len(dataset.mlb.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "checkpoint = torch.load(MODEL_PATH, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "last_epoch = checkpoint['epoch']\n",
    "epoch_loss_evolution = checkpoint['loss_evolution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj4bNrUvkv7P"
   },
   "source": [
    "## Load and classify by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rs = tl.GeoRaster2.open('../data/high_res_Para/analytic_2019-06_2019-11_mosaic/L15-0722E-1001N.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ffa6de8ccf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pydev_debug_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_raster_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9ffa6de8ccf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pydev_debug_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_raster_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/199/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTATE_SUSPEND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m                     \u001b[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/199/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# IFDEF CYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/199/plugins/python/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/199/plugins/python/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "image = preprocess_raster_image(rs)\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f8962a93ddb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Move channels from start of array (e.g. (C, H, W)) to the end (e.g. (H, W, C))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ijk->jki'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Stretch each channel to min/max for later converting the image to np.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rs' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'rs' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# Move channels from start of array (e.g. (C, H, W)) to the end (e.g. (H, W, C))\n",
    "image_original = np.einsum('ijk->jki',rs.image)[:, :, :3]\n",
    "\n",
    "# Stretch each channel to min/max for later converting the image to np.uint8\n",
    "image = image_original.astype(float)\n",
    "for idx_channel in range(image.shape[-1]):\n",
    "    image_min = image[..., idx_channel].min()\n",
    "    image_max = image[..., idx_channel].max()\n",
    "    image[..., idx_channel] = (image[..., idx_channel] - image_min) * (255/(image_max - image_min))\n",
    "    \n",
    "image = cv2.convertScaleAbs(image)\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "\n",
    "# Try equalizing the image (may look too enhanced and not natural)\n",
    "equ = np.zeros(image.shape, dtype=np.uint8)\n",
    "for idx_channel in range(image.shape[-1]):\n",
    "    equ[..., idx_channel] = cv2.equalizeHist(image[..., idx_channel])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(equ)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naJrhd_iF4Ol",
    "outputId": "f52f77a7-13fc-4589-cf6a-35e672269a32",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print('Test dataset size: ', len([name for name in os.listdir(TEST_DIR) if os.path.isfile(os.path.join(TEST_DIR, name))]))\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "dataset_test = ImageFolderWithPaths('test', test_transforms)\n",
    "dataloader = {'test': torch.utils.data.DataLoader(dataset_test, batch_size = 250, shuffle=False, num_workers=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJCggXb9tMwt",
    "outputId": "e9562734-90e0-4fd4-a68f-4b426591dafa",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(dataloader['test'].dataset)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8derxAyA6NHE",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def classify(model, dataloader, multi_label_binarizer, show_images=False, threshold=0):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    results_df = pd.DataFrame(columns=['image_name','tags'])\n",
    "    with tqdm(dataloader, unit=\"batch\") as classification_batch:\n",
    "      for inputs, _, img_names in classification_batch:\n",
    "        with torch.no_grad():\n",
    "          inputs = inputs.to(device)\n",
    "          outputs = model(inputs)\n",
    "          preds = outputs > threshold\n",
    "          # recover categorical labels from binary predictions\n",
    "          labels = multi_label_binarizer.inverse_transform(preds.cpu())\n",
    "\n",
    "          #labels is a list of size classification batch where each element \n",
    "          # is a tuple with the labels corresponding to each image. The \n",
    "          # submission expects the labels of each image to be outputted as a \n",
    "          # space separated list\n",
    "          output_labels = [' '.join(labels) for labels in labels]\n",
    "          \n",
    "          results_df = results_df.append(pd.DataFrame({\n",
    "              'image_name': img_names,\n",
    "              'tags': output_labels\n",
    "          }))\n",
    "\n",
    "          if show_images:\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            print('Labels: ', labels)\n",
    "            imshow(inputs.squeeze(0).cpu().data)\n",
    "    \n",
    "    model.train(mode=was_training)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcM6duuRk4sv",
    "outputId": "721cd61f-d0ba-45a3-d7ce-b466cfe04636",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results_df = classify(model, dataloader['test'], dataset.mlb, show_images=False, threshold=0.2)\n",
    "results_df.to_csv(SUBMISSION_FILENAME, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp2cRlpiTjRI"
   },
   "source": [
    "# Submitting results to Kaggle competition\n",
    "* Instructions on how to use Kaggle API from Google Colab environment can be found [here](https://colab.research.google.com/github/corrieann/kaggle/blob/master/kaggle_api_in_colab.ipynb)\n",
    "* Details on the format of the output that should be submitted as well as the Kaggle API command that should be used can be found [here](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QawwT_TaKVrO",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(SUBMISSION_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "n-IyaqU-Wji3",
    "outputId": "57f84d6c-b357-4db9-8473-c38d38dfaa28",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Dd1GMPa6HI3l",
    "autVzv_Il-E9",
    "Md-UGXawSUle",
    "Uk5xmHYcXT5g",
    "7ywJG9yeZ2kU",
    "MD-e-X03yNPN"
   ],
   "name": "Copy of Planet-Amazon challenge-Learning rate optimization.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lddm/ml-journey/blob/main/Planet_Amazon_challenge_Learning_rate_optimization.ipynb",
     "timestamp": 1618676097206
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c8424e4aae5485199b53d7a2443034e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "566eb001b14c43678ebe8c281a8662c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58a5037696124d84844fc92cf8552503": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_566eb001b14c43678ebe8c281a8662c4",
      "placeholder": "",
      "style": "IPY_MODEL_5f98920151ee46e59a405fd41038af3e",
      "value": " 97.8M/97.8M [00:14&lt;00:00, 7.30MB/s]"
     }
    },
    "5f98920151ee46e59a405fd41038af3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "605fef7441184dedb805816f48039fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83b141877a0c4de98d007704fed029f6",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa023ba766824a54823e71f6eb42636f",
      "value": 102502400
     }
    },
    "83b141877a0c4de98d007704fed029f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83d2a0b388984de4b394dbc2dc59e5af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_605fef7441184dedb805816f48039fa8",
       "IPY_MODEL_58a5037696124d84844fc92cf8552503"
      ],
      "layout": "IPY_MODEL_1c8424e4aae5485199b53d7a2443034e"
     }
    },
    "aa023ba766824a54823e71f6eb42636f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}